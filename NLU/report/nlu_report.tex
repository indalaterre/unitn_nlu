\documentclass[a4paper]{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{INTERSPEECH2021}

% Put the lab number of the corresponding exercise
\title{NLU course project - Lab5}
\name{Mauro Antonio de Palma (256175)}

\address{
  University of Trento}
\email{mauroantonio.depalma@studenti.unitn.it}

\begin{document}

\maketitle

\section{Introduction}
This report shows the results of the experiments in the context of \textbf{Slot filling} and \textbf{Intent classification}. These two concepts are bound together in a NLU scenario where \textit{Intent classification} allows the software to understand the main user intention while \textit{Slot filling} extracts the parameters related to the intention by mapping utterance tokens to domain-specific labels.
\begin{itemize}
	\item \textit{PartA} enhances an existing LSTM architecture adding bidirectionality and a dropout layer after the initial embedding layer and before the output layer.
	\item \textit{Part B} fine-tunes the BERT\cite{Chen19-BERT} model taking advantage of its pre-trained weights.
\end{itemize}

\section{Implementation details}
Both experiments use the ATIS (Airline Travel Information Systems) dataset. To avoid being too dependant on initial random weights the calculated metrics will be averaged across 5 runs of 100 epochs using AdamW optimizer (learning rate = 0.0001). Models are evaluated using \textbf{F1-Score} for slot filling and \textbf{Accuracy} for intent classification. The training objective is to minimize their \textbf{CrossEntropy} loss combined using \textit{Uncertainty weight}\cite{Kendal17-UncWeight} using the following formula:
\begin{equation}
	\mathcal{L} = \frac{1}{\sigma_i^2}\mathcal{L}_i + \frac{1}{\sigma_s^2}\mathcal{L}_s + \log(\sigma_i^2) + \log(\sigma_s^2)
\end{equation}
Where $\log(\sigma_i^2)$ and $\log(\sigma_s^2)$ are two learned parameters. $s = \log(\sigma^2)$ is used to ensure numerical stability (avoids 0-division and negative values for the variance) since $\sigma^2=e^s$.
This joint loss will be used as training/validation loss.

\subsection{Part A}
The baseline LSTM has been enhanced by incrementally applying some new design approaches and regularization techniques:
\begin{itemize}
	\item \textit{Bidirectionality}: In natural language each word takes its meaning from other words in both directions. Letting the model process data in both forward and backward directions allows it to get the full context and knowledge using all the available data
	\item \textit{Dropout}: Dropout randomly drop neurons with a probability \textit{p = 0.3} only at training time. This is useful to add uncertainty during the training time and avoid being stuck in local optima. This layer will be applied after the embedding and before the output layers.
\end{itemize}

\subsection{Part B}
This part fine-tunes the pre-trained BERT model for joint intent classification and slot filling. Both BERT-base (768-dimensional hidden states) and BERT-large (1024-dimensional) are evaluated.

\textbf{Intent classification}: the model leverages \textit{[CLS]} token representation from BERT's final transformer layer which encodes the entire utterance semantic meaning. A linear layer projects this 768-dimensional vector (1024 for BERT-large) to the 21 ATIS intent classes.

\textbf{Slot Filling}: To handle BERT's \textit{WordPiece} sub-tokenization, the preprocessing assigns each word's slot label only to its \textit{first} sub-token. Additional sub-tokens generated by WordPiece are padded with a special \texttt{pad} token. During training, \texttt{CrossEntropyLoss} ignores these padded positions. At evaluation time, only positions with non-pad ground truth labels are considered for the F1-score calculation, ensuring alignment between predictions and gold labels.

\section{Results}
The metrics evaluated are the accuracy for the intent classification and the F1 score for slot filling using the test dataset (never used during the training phase) Table \ref{tab:lstm} and Table \ref{tab:bert} show the results of all the experiments conducted in the first and second part. Image \ref{fig:uncertainty_chart}. shows an intresting chart about how the weight to combine slots/intents losses are learnt during the epoch.
\begin{itemize}
	\item \textbf{Part A}: The enhancements (bidirectionality and dropout) provide an incremental improvement in F1-score and accuracy over the baseline.
	
	\item \textbf{Part B}: Fine-tuning BERT yields significant performance gains, with BERT-Large offering a slight edge over BERT-Base due to its increased capacity.
\end{itemize} 

\begin{table}[h!]
	\centering
	
	\begin{tabularx}{\columnwidth}{Xcc}
		\toprule
		\textbf{Architecture} & \textbf{Slots F1} & \textbf{Intents Acc.} \\
		
		\midrule
		LSTM & 0.8983 ± 0.0061 & 0.9342 ± 0.0023 \\
		LSTM bidirectional & 0.93 ± 0.0033 & 0.9445 ± 0.0017 \\
		LSTM bidirectional + dropout & 0.926 ± 0.0029 & 0.951 ± 0.0013 \\
		
		\hline
	\end{tabularx}
	\vspace{0.2cm}
	\caption{LSTM model with bidirectionality and dropout}
	\label{tab:lstm}
\end{table}

\begin{figure}[h!]
	\centering
	\includegraphics[width=\columnwidth]{images/uncertainty_learning.png}
	\caption{Evolution of uncertainty weights during training.}
	\label{fig:uncertainty_chart}
\end{figure}

\begin{table}[h!]
	\centering
	
	\begin{tabularx}{\columnwidth}{Xcc}
		\toprule
		\textbf{Architecture} & \textbf{Slots F1} & \textbf{Intents Acc.} \\
		
		\midrule
		BERT-base & 0.8983 ± 0.0061 & 0.9342 ± 0.0023 \\
		BERT-large & 0.93 ± 0.0033 & 0.9445 ± 0.0017 \\		
		\hline
	\end{tabularx}
	\vspace{0.2cm}
	\caption{BERT fine-tuning results}
	\label{tab:bert}
\end{table}

\bibliographystyle{IEEEtran}

\bibliography{mybib}


\end{document}
