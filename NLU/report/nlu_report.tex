\documentclass[a4paper]{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{INTERSPEECH2021}

% Put the lab number of the corresponding exercise
\title{NLU course project - Lab5}
\name{Mauro Antonio de Palma (256175)}

\address{
  University of Trento}
\email{mauroantonio.depalma@studenti.unitn.it}

\begin{document}

\maketitle

\section{Introduction}
This report shows the results of the experiments in the context of \textbf{Slot filling} and \textbf{Intent classification}. These two concepts are bound together in a NLU scenario where \textit{Intent classification} allows the software to understand the main user intention while \textit{Slot filling} extracts the parameters related to the intention by mapping utterance tokens to domain-specific labels.
\begin{itemize}
	\item \textit{PartA} enhances an existing LSTM architecture adding bidirectionality and a dropout layer after the initial embedding layer and before the output layer.
	\item \textit{Part B} fine-tunes the BERT\cite{Chen19-BERT} model taking advantage of its pre-trained weights.
\end{itemize}

\section{Implementation details}
Both experiments use the ATIS (Airline Travel Information Systems) dataset. To avoid being too dependant on initial random weights the calculated metrics will be averaged across 5 runs of 100 epochs using AdamW optimizer (learning rate = 0.0001). Models are evaluated using \textbf{F1-Score} for slot filling and \textbf{Accuracy} for intent classification. The training objective is to minimize their \textbf{CrossEntropy} loss combined using \textit{Uncertainty weight}\cite{Kendal17-UncWeight} by the following formula:
\begin{equation}
	\mathcal{L} = \frac{1}{\sigma_i^2}\mathcal{L}_i + \frac{1}{\sigma_s^2}\mathcal{L}_s + \log(\sigma_i^2) + \log(\sigma_s^2)
\end{equation}
Where $\log(\sigma_i^2)$ and $\log(\sigma_s^2)$ are two learned parameters. $s = \log(\sigma^2)$ is used to ensure numerical stability (avoids 0-division and negative values for the variance) since $\sigma^2=e^s$.
This joint loss will be used as training/validation loss.

\subsection{Part A}
The baseline LSTM has been enhanced by incrementally applying some new design approaches and regularization techniques:
\begin{itemize}
	\item \textit{Bidirectionality}: In natural language each word takes its meaning from other words in both directions. Letting the model process data in both forward and backward directions allows it to get the full context and knowledge using all the available data
	\item \textit{Dropout}: Dropout randomly drop neurons with a probability \textit{p = 0.3} only at training time. This is useful to add uncertainty at training time and avoid being stuck in local optima. This layer will be applied after the embedding and before the output layers.
\end{itemize}

\subsection{Part B}
This part fine-tunes the pre-trained BERT model for joint intent classification and slot filling. Both BERT-base (768-dimensional hidden states) and BERT-large (1024-dimensional hidden states) are evaluated.

\textbf{Intent classification}: the model leverages \textit{[CLS]} token representation from BERT's final transformer layer which encodes the entire utterance semantic meaning. A linear layer projects this 768-dimensional vector (1024 for BERT-large) to the 21 ATIS intent classes.

\textbf{Slot Filling}: To handle BERT's \textit{WordPiece} sub-tokenization, the preprocessing assigns each word's slot label only to its \textit{first} sub-token. Additional sub-tokens generated by WordPiece are padded with a special \texttt{pad} token. During training, \texttt{CrossEntropyLoss} ignores these padded positions. At evaluation time, only positions with non-pad ground truth labels are considered for the F1-score calculation, ensuring alignment between predictions and gold labels.

\section{Results}
The metrics evaluated are the accuracy for the intent classification and the F1 score for slot filling using the test dataset (never used during the training phase) Table \ref{tab:lstm} and Table \ref{tab:bert} show the results of all the experiments conducted in the first and second part.
\begin{itemize}
	\item \textbf{Part A}: The enhancements (bidirectionality and dropout) provide an incremental improvement in F1-score and accuracy over the baseline.
	
	\item \textbf{Part B}: BERT-Large offers a slight edge in Slot Filling, though Intent Accuracy remains comparable (or slightly lower) than the Base model, suggesting diminishing returns for simple classification tasks.
\end{itemize} 
Figure \ref{fig:uncertainty_chart} and Figure \ref{fig:bert_uncertainty_chart} illustrates the dynamic tuning of the loss weights during training. While the weights for both tasks increase over time, we observe a distinct divergence between the two. The weight for intent classification (blue) grows at a faster rate and reaches a higher final magnitude than the weight for slot filling (orange). Consequently, the intent classification task effectively holds more weight in the final aggregate loss calculation compared to the slot filling task.

\bibliographystyle{IEEEtran}

\bibliography{mybib}

\begin{table}[h!]
	\centering
	\caption{LSTM model with bidirectionality and dropout}
	\label{tab:lstm}
	\begin{tabularx}{\columnwidth}{Xcc}
		\toprule
		\textbf{Architecture} & \textbf{Slots F1} & \textbf{Intents Acc.} \\
		
		\midrule
		LSTM & 0.8997 ± 0.0041 & 0.9335 ± 0.0057 \\
		LSTM bidirectional & 0.9282 ± 0.0024 & 0.9431 ± 0.0041 \\
		LSTM bidirectional + dropout & 0.9255 ± 0.0034 & 0.9440 ± 0.0025 \\
		
		\hline
	\end{tabularx}
\end{table}

\begin{figure}[h!]
	\centering
	\caption{Growth of uncertainty weights during LSTM training.}
	\label{fig:uncertainty_chart}
	\includegraphics[width=\columnwidth]{images/uncertainty_learning.png}
\end{figure}

\begin{table}[h!]
	\centering
	\caption{BERT fine-tuning results}
	\label{tab:bert}
	\begin{tabularx}{\columnwidth}{Xcc}
		\toprule
		\textbf{Architecture} & \textbf{Slots F1} & \textbf{Intents Acc.} \\
		
		\midrule
		BERT-base & 0.9540 ± 0.0025 & 0.9760 ± 0.0017 \\
		BERT-large & 0.9571 ± 0.0022 & 0.9722 ± 0.0019 \\		
		\hline
	\end{tabularx}
\end{table}

\begin{figure}[h!]
	\centering
	\caption{Growth of BERT-large uncertainty weights.}
	\label{fig:bert_uncertainty_chart}
	\includegraphics[width=\columnwidth]{images/bert_uncertainty_learning.png}
\end{figure}


\end{document}
